{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7d9f37e8",
      "metadata": {
        "id": "7d9f37e8"
      },
      "source": [
        "# ‚öôÔ∏è SAM 2 Industrial Object Tracking & YOLO Annotation\n",
        "\n",
        "Notebook n√†y s·ª≠ d·ª•ng **SAM 2 (Segment Anything Model 2)** ƒë·ªÉ:\n",
        "1. Segment v√† tracking **c√°c ƒë·ªëi t∆∞·ª£ng/s·∫£n ph·∫©m** tr√™n bƒÉng truy·ªÅn t·ª´ video\n",
        "2. T·∫°o annotation dataset theo format YOLO ƒë·ªÉ training model detection/segmentation cho ƒë·ªëi t∆∞·ª£ng n√†y\n",
        "\n",
        "## Workflow:\n",
        "1. **Setup** - C√†i ƒë·∫∑t SAM 2 v√† c√°c dependencies\n",
        "2. **Video Processing** - Extract video th√†nh frames\n",
        "3. **Interactive Annotation** - Click ch·ªçn c√°c ƒë·ªëi t∆∞·ª£ng/s·∫£n ph·∫©m tr√™n 1 frame\n",
        "4. **Propagation** - SAM 2 t·ª± ƒë·ªông track qua to√†n b·ªô video\n",
        "5. **Export** - Xu·∫•t dataset theo format YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "029bc0cc",
      "metadata": {
        "id": "029bc0cc"
      },
      "source": [
        "## 1. Setup m√¥i tr∆∞·ªùng\n",
        "\n",
        "C√†i ƒë·∫∑t SAM 2 v√† c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt. Cell n√†y s·∫Ω t·ª± ƒë·ªông detect m√¥i tr∆∞·ªùng Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634e4970",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "634e4970",
        "outputId": "708633a4-cc00-4c6e-bb13-5087a3dc8207"
      },
      "outputs": [],
      "source": [
        "# Detect m√¥i tr∆∞·ªùng Colab\n",
        "import os\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"‚úÖ ƒêang ch·∫°y tr√™n Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"‚ÑπÔ∏è ƒêang ch·∫°y local (kh√¥ng ph·∫£i Colab)\")\n",
        "\n",
        "# C√†i ƒë·∫∑t SAM 2 v√† dependencies\n",
        "if IN_COLAB:\n",
        "    # C√†i ƒë·∫∑t SAM 2 t·ª´ source\n",
        "    !pip install -q 'git+https://github.com/facebookresearch/sam2.git'\n",
        "    !pip install -q opencv-python matplotlib ipympl\n",
        "\n",
        "    # T·∫£i checkpoint model\n",
        "    !mkdir -p checkpoints\n",
        "    !wget -q -P checkpoints https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "    print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t SAM 2 v√† t·∫£i checkpoint\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Local mode - ƒë·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t SAM 2 v√† c√≥ checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d33568d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d33568d",
        "outputId": "082af27a-2c84-4a93-e1a0-3b04f0486869"
      },
      "outputs": [],
      "source": [
        "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"‚ö†Ô∏è GPU kh√¥ng kh·∫£ d·ª•ng, s·ª≠ d·ª•ng CPU (s·∫Ω ch·∫≠m h∆°n)\")\n",
        "\n",
        "# C·∫•u h√¨nh dtype cho memory efficiency\n",
        "if device.type == \"cuda\":\n",
        "    # S·ª≠ d·ª•ng bfloat16 n·∫øu GPU h·ªó tr·ª£\n",
        "    if torch.cuda.get_device_capability()[0] >= 8:\n",
        "        torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "        if torch.cuda.get_device_properties(0).major >= 8:\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "            print(\"‚úÖ ƒê√£ b·∫≠t TF32 v√† bfloat16 ƒë·ªÉ tƒÉng hi·ªáu su·∫•t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc2b09b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fc2b09b",
        "outputId": "e6f33ef2-d45d-4157-d6d8-e47fe7dae44c"
      },
      "outputs": [],
      "source": [
        "# Import SAM 2\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "# C·∫•u h√¨nh model\n",
        "sam2_checkpoint = \"checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "# Kh·ªüi t·∫°o predictor\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
        "print(\"‚úÖ ƒê√£ load SAM 2 model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62b2395",
      "metadata": {
        "id": "a62b2395"
      },
      "source": [
        "## 2. Video Processing\n",
        "\n",
        "Upload video v√† extract th√†nh c√°c frame JPEG. SAM 2 y√™u c·∫ßu frames ƒë∆∞·ª£c l∆∞u d∆∞·ªõi d·∫°ng file ·∫£nh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0664a010",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "0664a010",
        "outputId": "0f5e5a84-82da-4137-cdd1-65a9bfc1dc6b"
      },
      "outputs": [],
      "source": [
        "# Upload video (Colab) ho·∫∑c ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n (local)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    print(\"üì§ Vui l√≤ng upload video c·ªßa b·∫°n:\")\n",
        "    uploaded = files.upload()\n",
        "    video_path = list(uploaded.keys())[0]\n",
        "    print(f\"‚úÖ ƒê√£ upload: {video_path}\")\n",
        "else:\n",
        "    # ƒê∆∞·ªùng d·∫´n video local - THAY ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N N√ÄY\n",
        "    video_path = \"industrial_video.mp4\"  # <-- Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n video c·ªßa b·∫°n\n",
        "    print(f\"üìπ S·ª≠ d·ª•ng video: {video_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29001e3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29001e3a",
        "outputId": "8eaaf40e-8d3d-4ce7-cb2c-0c4ef592e910"
      },
      "outputs": [],
      "source": [
        "# Extract frames t·ª´ video\n",
        "frames_dir = \"frames_for_object\"\n",
        "os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "# C·∫•u h√¨nh extract\n",
        "FRAME_RATE = 5  # S·ªë frame m·ªói gi√¢y (gi·∫£m n·∫øu video d√†i ƒë·ªÉ ti·∫øt ki·ªám memory)\n",
        "\n",
        "# S·ª≠ d·ª•ng ffmpeg ƒë·ªÉ extract frames\n",
        "!ffmpeg -i \"{video_path}\" -vf \"fps={FRAME_RATE}\" -q:v 2 -start_number 0 \"{frames_dir}/%05d.jpg\" -y -loglevel quiet\n",
        "\n",
        "# ƒê·∫øm s·ªë frames ƒë√£ extract\n",
        "frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith('.jpg')])\n",
        "num_frames = len(frame_files)\n",
        "print(f\"‚úÖ ƒê√£ extract {num_frames} frames v√†o th∆∞ m·ª•c '{frames_dir}/'\")\n",
        "\n",
        "# Hi·ªÉn th·ªã th√¥ng tin video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "duration = total_frames / original_fps\n",
        "cap.release()\n",
        "print(f\"üìπ Video g·ªëc: {original_fps:.1f} FPS, {total_frames} frames, {duration:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bdbd6cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "4bdbd6cb",
        "outputId": "31d91a6b-e24e-4588-f258-0054f6df8f9b"
      },
      "outputs": [],
      "source": [
        "# Preview frame ƒë·∫ßu ti√™n\n",
        "first_frame_path = os.path.join(frames_dir, frame_files[0])\n",
        "first_frame = Image.open(first_frame_path)\n",
        "frame_width, frame_height = first_frame.size\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(first_frame)\n",
        "plt.title(f\"Frame ƒë·∫ßu ti√™n - Size: {frame_width}x{frame_height}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìê K√≠ch th∆∞·ªõc frame: {frame_width} x {frame_height} pixels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8c30d4",
      "metadata": {
        "id": "bf8c30d4"
      },
      "source": [
        "## 3. Annotation - Nh·∫≠p T·ªça ƒê·ªô ƒê·ªëi T∆∞·ª£ng Tr√™n Nhi·ªÅu Frame\n",
        "\n",
        "H·ªó tr·ª£ annotation tr√™n **nhi·ªÅu frame** kh√°c nhau ƒë·ªÉ theo d√µi c√°c object m·ªõi xu·∫•t hi·ªán.\n",
        "\n",
        "**C√°ch s·ª≠ d·ª•ng:**\n",
        "- Xem ·∫£nh frame v·ªõi grid t·ªça ƒë·ªô chi ti·∫øt (tr·ª•c X m√†u ƒë·ªè, tr·ª•c Y m√†u xanh)\n",
        "- X√°c ƒë·ªãnh v·ªã tr√≠ **t√¢m** c·ªßa m·ªói ƒë·ªëi t∆∞·ª£ng\n",
        "- Khai b√°o annotation theo format: `{frame_idx: [(x1, y1), (x2, y2), ...]}`\n",
        "- M·ªói frame c√≥ th·ªÉ c√≥ nhi·ªÅu object, v√† b·∫°n c√≥ th·ªÉ th√™m object m·ªõi ·ªü c√°c frame sau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713b6fde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "713b6fde",
        "outputId": "d2b665d4-3643-4db2-b38b-055e08540901"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# C·∫§U H√åNH C√ÅC FRAME C·∫¶N ANNOTATION\n",
        "# ============================================================\n",
        "# Danh s√°ch c√°c frame index c·∫ßn hi·ªÉn th·ªã ƒë·ªÉ annotation\n",
        "# Th√™m c√°c frame m√† c√≥ object M·ªöI xu·∫•t hi·ªán\n",
        "\n",
        "ANNOTATION_FRAMES = list(range(0, num_frames, 10))  # Thay ƒë·ªïi theo nhu c·∫ßu c·ªßa b·∫°n\n",
        "\n",
        "# ============================================================\n",
        "\n",
        "# Hi·ªÉn th·ªã t·ª´ng frame v·ªõi grid t·ªça ƒë·ªô\n",
        "for frame_idx in ANNOTATION_FRAMES:\n",
        "    if frame_idx >= len(frame_files):\n",
        "        print(f\"‚ö†Ô∏è Frame {frame_idx} kh√¥ng t·ªìn t·∫°i (ch·ªâ c√≥ {len(frame_files)} frames)\")\n",
        "        continue\n",
        "\n",
        "    annotation_frame_path = os.path.join(frames_dir, frame_files[frame_idx])\n",
        "    annotation_image = np.array(Image.open(annotation_frame_path))\n",
        "\n",
        "    h, w = annotation_image.shape[:2]\n",
        "\n",
        "    # V·∫Ω ·∫£nh v·ªõi grid t·ªça ƒë·ªô chi ti·∫øt\n",
        "    fig, ax = plt.subplots(figsize=(16, 12))\n",
        "    ax.imshow(annotation_image)\n",
        "\n",
        "    # Grid lines - m·ªói 50 pixels\n",
        "    grid_step = 50\n",
        "    for x in range(0, w + 1, grid_step):\n",
        "        ax.axvline(x, color='cyan', alpha=0.4, linewidth=0.5)\n",
        "    for y in range(0, h + 1, grid_step):\n",
        "        ax.axhline(y, color='cyan', alpha=0.4, linewidth=0.5)\n",
        "\n",
        "    # Labels t·ªça ƒë·ªô X (tr√™n c√πng v√† d∆∞·ªõi c√πng) - m·ªói 100 pixels\n",
        "    for x in range(0, w + 1, 100):\n",
        "        ax.text(x, -15, str(x), color='red', fontsize=9, ha='center', fontweight='bold')\n",
        "        ax.text(x, h + 15, str(x), color='red', fontsize=9, ha='center', fontweight='bold')\n",
        "\n",
        "    # Labels t·ªça ƒë·ªô Y (b√™n tr√°i v√† b√™n ph·∫£i) - m·ªói 100 pixels\n",
        "    for y in range(0, h + 1, 100):\n",
        "        ax.text(-25, y, str(y), color='blue', fontsize=9, va='center', fontweight='bold')\n",
        "        ax.text(w + 25, y, str(y), color='blue', fontsize=9, va='center', fontweight='bold')\n",
        "\n",
        "    # Th√™m cross-hair t·∫°i t√¢m ƒë·ªÉ tham chi·∫øu\n",
        "    center_x, center_y = w // 2, h // 2\n",
        "    ax.axvline(center_x, color='yellow', alpha=0.5, linewidth=1, linestyle='--')\n",
        "    ax.axhline(center_y, color='yellow', alpha=0.5, linewidth=1, linestyle='--')\n",
        "    ax.plot(center_x, center_y, 'y+', markersize=20, markeredgewidth=2)\n",
        "\n",
        "    ax.set_xlim(-50, w + 50)\n",
        "    ax.set_ylim(h + 50, -50)  # Inverted Y axis\n",
        "    ax.set_xlabel(\"X coordinate (pixels)\", fontsize=12, color='red', fontweight='bold')\n",
        "    ax.set_ylabel(\"Y coordinate (pixels)\", fontsize=12, color='blue', fontweight='bold')\n",
        "    ax.set_title(f\"üéØ FRAME {frame_idx} - X√°c ƒë·ªãnh t·ªça ƒë·ªô X,Y c·ªßa ƒë·ªëi t∆∞·ª£ng\\n\"\n",
        "                 f\"K√≠ch th∆∞·ªõc: {w}x{h} pixels | Grid: 50px\", fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"üì∑ Frame {frame_idx}: {frame_files[frame_idx]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìù H∆Ø·ªöNG D·∫™N KHAI B√ÅO T·ªåA ƒê·ªò (MULTI-FRAME):\")\n",
        "print(\"   Xem cell ti·∫øp theo ƒë·ªÉ khai b√°o t·ªça ƒë·ªô cho t·ª´ng frame\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5bb888",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e5bb888",
        "outputId": "218e1101-d712-42cf-962f-ec410483b4b6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# NH·∫¨P T·ªåA ƒê·ªò ƒê·ªêI T∆Ø·ª¢NG TR√äN NHI·ªÄU FRAME\n",
        "# ============================================================\n",
        "# Format: { frame_idx: [(x1, y1), (x2, y2), ...] }\n",
        "#\n",
        "# - frame_idx: S·ªë th·ª© t·ª± frame (b·∫Øt ƒë·∫ßu t·ª´ 0)\n",
        "# - (x, y): T·ªça ƒë·ªô t√¢m c·ªßa ƒë·ªëi t∆∞·ª£ng\n",
        "#\n",
        "# üí° L∆ØU √ù QUAN TR·ªåNG:\n",
        "# - Frame 0: ƒê√°nh d·∫•u T·∫§T C·∫¢ objects hi·ªán c√≥ trong frame ƒë·∫ßu ti√™n\n",
        "# - Frame 10, 20, ...: Ch·ªâ c·∫ßn ƒë√°nh d·∫•u c√°c objects M·ªöI xu·∫•t hi·ªán\n",
        "# - SAM 2 s·∫Ω t·ª± ƒë·ªông track objects ƒë√£ ƒë∆∞·ª£c ƒë√°nh d·∫•u ·ªü c√°c frame tr∆∞·ªõc\n",
        "# - Object ID s·∫Ω ƒë∆∞·ª£c g√°n t·ª± ƒë·ªông theo th·ª© t·ª± khai b√°o (1, 2, 3, ...)\n",
        "\n",
        "multi_frame_annotations = {\n",
        "    # ============ FRAME 0 - Objects ban ƒë·∫ßu ============\n",
        "    0: [\n",
        "        (950,600),\n",
        "        (550,450),\n",
        "        (350,350),\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "        # ============ FRAME 10 - Objects m·ªõi xu·∫•t hi·ªán ============\n",
        "    # 10: [\n",
        "    #     # (x, y),   # Object m·ªõi 1\n",
        "    #     # (x, y),   # Object m·ªõi 2\n",
        "    # ],\n",
        "\n",
        "    # ============ FRAME 20 - Objects m·ªõi xu·∫•t hi·ªán ============\n",
        "    # 20: [\n",
        "    #     # (x, y),   # Object m·ªõi\n",
        "    # ],\n",
        "\n",
        "    # ============ Th√™m c√°c frame kh√°c n·∫øu c·∫ßn ============\n",
        "    # 30: [\n",
        "    #     # (x, y),\n",
        "    # ],\n",
        "\n",
        "    20: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "    40: [\n",
        "        (275,250)\n",
        "    ],\n",
        "\n",
        "    50: [\n",
        "        (250,200)\n",
        "    ],\n",
        "\n",
        "    70: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "    80: [\n",
        "        (225,200)\n",
        "    ],\n",
        "\n",
        "    100: [\n",
        "        (250,200)\n",
        "    ],\n",
        "\n",
        "    120: [\n",
        "        (250,200)\n",
        "    ],\n",
        "\n",
        "    130: [\n",
        "        (200,200)\n",
        "    ],\n",
        "\n",
        "    140: [\n",
        "        (200,200)\n",
        "    ],\n",
        "\n",
        "    160: [\n",
        "        (200,200)\n",
        "    ],\n",
        "\n",
        "    190: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "    200: [\n",
        "        (225,200)\n",
        "    ],\n",
        "\n",
        "    220: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "    240: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "    270: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "    290: [\n",
        "        (275,250)\n",
        "    ],\n",
        "\n",
        "    300: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "    320: [\n",
        "        (250,250)\n",
        "    ],\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# KI·ªÇM TRA V√Ä T·ªîNG H·ª¢P ANNOTATIONS\n",
        "# ============================================================\n",
        "\n",
        "# ƒê·∫øm t·ªïng s·ªë objects v√† ki·ªÉm tra\n",
        "total_objects = 0\n",
        "object_counter = 0\n",
        "all_prompts = {}  # {obj_id: {\"frame_idx\": idx, \"point\": (x, y)}}\n",
        "\n",
        "print(\"üìä T·ªîNG H·ª¢P ANNOTATIONS:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for frame_idx in sorted(multi_frame_annotations.keys()):\n",
        "    points = multi_frame_annotations[frame_idx]\n",
        "    if not points:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüéØ Frame {frame_idx}:\")\n",
        "    for i, (x, y) in enumerate(points):\n",
        "        object_counter += 1\n",
        "        all_prompts[object_counter] = {\n",
        "            \"frame_idx\": frame_idx,\n",
        "            \"point\": (x, y)\n",
        "        }\n",
        "        print(f\"   Object {object_counter}: ({x}, {y})\")\n",
        "\n",
        "    total_objects += len(points)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if total_objects == 0:\n",
        "    print(\"‚ö†Ô∏è CH∆ØA NH·∫¨P T·ªåA ƒê·ªò N√ÄO!\")\n",
        "    print(\"   Vui l√≤ng ƒëi·ªÅn t·ªça ƒë·ªô v√†o dictionary 'multi_frame_annotations' ·ªü tr√™n\")\n",
        "    print(\"\\n   V√≠ d·ª•:\")\n",
        "    print(\"   multi_frame_annotations = {\")\n",
        "    print(\"       0: [(150, 200), (350, 180)],   # 2 objects ·ªü frame 0\")\n",
        "    print(\"       10: [(500, 300)],              # 1 object m·ªõi ·ªü frame 10\")\n",
        "    print(\"   }\")\n",
        "else:\n",
        "    print(f\"‚úÖ T·ªïng c·ªông: {total_objects} objects tr√™n {len([k for k, v in multi_frame_annotations.items() if v])} frames\")\n",
        "    print(f\"   Object IDs: 1 ‚Üí {object_counter}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18917e56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "18917e56",
        "outputId": "04348d3c-5b62-4624-dec4-5aa20c0eae62"
      },
      "outputs": [],
      "source": [
        "# Preview c√°c ƒëi·ªÉm ƒë√£ ch·ªçn tr√™n t·ª´ng frame\n",
        "if total_objects > 0:\n",
        "    # Nh√≥m objects theo frame\n",
        "    frames_to_show = sorted(set(p[\"frame_idx\"] for p in all_prompts.values()))\n",
        "\n",
        "    for frame_idx in frames_to_show:\n",
        "        frame_path = os.path.join(frames_dir, frame_files[frame_idx])\n",
        "        frame_image = np.array(Image.open(frame_path))\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(14, 10))\n",
        "        ax.imshow(frame_image)\n",
        "\n",
        "        # V·∫Ω c√°c objects ƒë∆∞·ª£c annotate T·∫†I frame n√†y\n",
        "        objects_in_frame = [(obj_id, info) for obj_id, info in all_prompts.items()\n",
        "                           if info[\"frame_idx\"] == frame_idx]\n",
        "\n",
        "        for obj_id, info in objects_in_frame:\n",
        "            x, y = info[\"point\"]\n",
        "            # V·∫Ω marker\n",
        "            ax.plot(x, y, 'r*', markersize=20, markeredgecolor='white', markeredgewidth=1)\n",
        "            ax.add_patch(plt.Circle((x, y), 25, fill=False, color='lime', linewidth=2))\n",
        "            # Label\n",
        "            ax.text(x + 30, y - 10, f\"Obj {obj_id}\\n({x},{y})\", color='yellow', fontsize=10,\n",
        "                    fontweight='bold', bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
        "\n",
        "        ax.set_title(f\"‚úÖ Frame {frame_idx}: {len(objects_in_frame)} object(s) ƒë∆∞·ª£c annotate t·∫°i ƒë√¢y\", fontsize=14)\n",
        "        ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    print(f\"\\n‚öôÔ∏è T·ªïng c·ªông {total_objects} objects s·∫Ω ƒë∆∞·ª£c track b·ªüi SAM 2\")\n",
        "    print(\"   Ti·∫øp t·ª•c ch·∫°y c√°c cell ti·∫øp theo ƒë·ªÉ segment v√† propagate.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Ch∆∞a c√≥ t·ªça ƒë·ªô n√†o. H√£y nh·∫≠p t·ªça ƒë·ªô v√†o cell tr∆∞·ªõc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ce7670",
      "metadata": {
        "id": "09ce7670"
      },
      "source": [
        "## 4. SAM 2 Segmentation & Tracking\n",
        "\n",
        "S·ª≠ d·ª•ng SAM 2 ƒë·ªÉ segment c√°c ƒë·ªëi t∆∞·ª£ng ƒë√£ click v√† propagate qua to√†n b·ªô video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cafdacb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cafdacb",
        "outputId": "cf863fc2-3f9f-4ff1-c9ac-0bf85d8e36e1"
      },
      "outputs": [],
      "source": [
        "# Kh·ªüi t·∫°o SAM 2 inference state\n",
        "inference_state = predictor.init_state(video_path=frames_dir)\n",
        "print(f\"‚úÖ ƒê√£ kh·ªüi t·∫°o inference state v·ªõi {num_frames} frames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43cc12a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43cc12a5",
        "outputId": "0c5b5f98-d3bf-4b78-e300-3a0d686f9226"
      },
      "outputs": [],
      "source": [
        "# Th√™m c√°c prompt clicks cho m·ªói ƒë·ªëi t∆∞·ª£ng (H·ªñ TR·ª¢ MULTI-FRAME)\n",
        "predictor.reset_state(inference_state)\n",
        "\n",
        "prompts = {}  # L∆∞u tr·ªØ prompts ƒë·ªÉ visualize\n",
        "\n",
        "for obj_id, info in all_prompts.items():\n",
        "    frame_idx = info[\"frame_idx\"]\n",
        "    x, y = info[\"point\"]\n",
        "\n",
        "    # M·ªói click l√† m·ªôt positive point cho object ri√™ng bi·ªát\n",
        "    points = np.array([[x, y]], dtype=np.float32)\n",
        "    labels = np.array([1], dtype=np.int32)  # 1 = positive click\n",
        "\n",
        "    # Th√™m prompt v√†o SAM 2 T·∫†I FRAME T∆Ø∆†NG ·ª®NG\n",
        "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "        inference_state=inference_state,\n",
        "        frame_idx=frame_idx,  # Frame m√† object xu·∫•t hi·ªán\n",
        "        obj_id=obj_id,\n",
        "        points=points,\n",
        "        labels=labels,\n",
        "    )\n",
        "\n",
        "    prompts[obj_id] = {\"frame_idx\": frame_idx, \"point\": (x, y)}\n",
        "    print(f\"‚úÖ Object {obj_id} t·∫°i frame {frame_idx}: ({x:.1f}, {y:.1f})\")\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è T·ªïng c·ªông {len(all_prompts)} objects ƒë√£ ƒë∆∞·ª£c th√™m v√†o SAM 2\")\n",
        "print(f\"   Tr√™n {len(set(p['frame_idx'] for p in all_prompts.values()))} frames kh√°c nhau\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb2c9c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bdb2c9c9",
        "outputId": "3c8a310b-6ee3-419a-f3a1-aca0f1456094"
      },
      "outputs": [],
      "source": [
        "# H√†m hi·ªÉn th·ªã mask\n",
        "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        cmap = plt.cm.get_cmap(\"tab20\")\n",
        "        color = np.array([*cmap(obj_id % 20)[:3], 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=200):\n",
        "    pos_points = coords[labels == 1]\n",
        "    neg_points = coords[labels == 0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*',\n",
        "               s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*',\n",
        "               s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "# Preview segmentation tr√™n C√ÅC FRAME ƒê√É ANNOTATE\n",
        "annotated_frames = sorted(set(p[\"frame_idx\"] for p in prompts.values()))\n",
        "\n",
        "for frame_idx in annotated_frames:\n",
        "    frame_path = os.path.join(frames_dir, frame_files[frame_idx])\n",
        "    frame_image = np.array(Image.open(frame_path))\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    plt.imshow(frame_image)\n",
        "    plt.title(f\"Preview Segmentation - Frame {frame_idx}\")\n",
        "\n",
        "    # L·∫•y c√°c objects ƒë∆∞·ª£c annotate t·∫°i frame n√†y\n",
        "    objects_at_frame = {obj_id: info for obj_id, info in prompts.items()\n",
        "                        if info[\"frame_idx\"] == frame_idx}\n",
        "\n",
        "    # V·∫Ω masks v√† points\n",
        "    for obj_id, prompt in objects_at_frame.items():\n",
        "        # L·∫•y mask t·ª´ SAM 2\n",
        "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "            inference_state=inference_state,\n",
        "            frame_idx=frame_idx,\n",
        "            obj_id=obj_id,\n",
        "            points=np.array([[prompt[\"point\"][0], prompt[\"point\"][1]]], dtype=np.float32),\n",
        "            labels=np.array([1], dtype=np.int32),\n",
        "        )\n",
        "\n",
        "        # T√¨m index c·ªßa obj_id trong output\n",
        "        if hasattr(out_obj_ids, 'cpu'):\n",
        "            obj_ids_np = out_obj_ids.cpu().numpy()\n",
        "        else:\n",
        "            obj_ids_np = np.array(out_obj_ids)\n",
        "\n",
        "        idx = np.where(obj_ids_np == obj_id)[0]\n",
        "        if len(idx) > 0:\n",
        "            idx = idx[0]\n",
        "            mask = (out_mask_logits[idx] > 0.0).cpu().numpy()\n",
        "            show_mask(mask, plt.gca(), obj_id)\n",
        "\n",
        "        # V·∫Ω point\n",
        "        x, y = prompt[\"point\"]\n",
        "        plt.plot(x, y, 'g*', markersize=15)\n",
        "        plt.text(x + 5, y - 5, f\"Obj {obj_id}\", color='white', fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"‚úÖ Frame {frame_idx}: {len(objects_at_frame)} objects\")\n",
        "\n",
        "print(f\"\\n‚úÖ Preview segmentation ho√†n t·∫•t cho {len(annotated_frames)} frames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ea6527",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07ea6527",
        "outputId": "191fa42f-c428-44b8-abd6-f223d732f07f"
      },
      "outputs": [],
      "source": [
        "# Propagate masks qua to√†n b·ªô video - v·ªõi x·ª≠ l√Ω l·ªói cho multi-frame annotation\n",
        "# T·ªêI ∆ØU: M·ªói object ch·ªâ propagate trong kho·∫£ng frame gi·ªõi h·∫°n\n",
        "\n",
        "print(\"üîÑ ƒêang propagate masks qua to√†n b·ªô video...\")\n",
        "print(\"   (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t t√πy thu·ªôc v√†o ƒë·ªô d√†i video)\")\n",
        "\n",
        "video_segments = {}  # {frame_idx: {obj_id: mask}}\n",
        "\n",
        "# ============================================================\n",
        "# C·∫§U H√åNH\n",
        "# ============================================================\n",
        "FRAMES_PER_OBJECT = 65  # S·ªë frames t·ªëi ƒëa ƒë·ªÉ propagate cho m·ªói object\n",
        "\n",
        "# ============================================================\n",
        "# PH∆Ø∆†NG PH√ÅP 1: C·ªë g·∫Øng propagate b√¨nh th∆∞·ªùng (c√°ch g·ªëc)\n",
        "# ============================================================\n",
        "try:\n",
        "    print(\"\\nüìå Th·ª≠ ph∆∞∆°ng ph√°p 1: Propagate to√†n b·ªô video c√πng l√∫c...\")\n",
        "\n",
        "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "        frame_masks = {}\n",
        "        for i in range(len(out_obj_ids)):\n",
        "            obj_id = out_obj_ids[i]\n",
        "            if hasattr(obj_id, 'item'):\n",
        "                obj_id = obj_id.item()\n",
        "            else:\n",
        "                obj_id = int(obj_id)\n",
        "\n",
        "            mask = (out_mask_logits[i] > 0.0).cpu().numpy().squeeze()\n",
        "            frame_masks[obj_id] = mask\n",
        "\n",
        "        video_segments[out_frame_idx] = frame_masks\n",
        "\n",
        "        if (out_frame_idx + 1) % 10 == 0 or out_frame_idx == num_frames - 1:\n",
        "            print(f\"   Processed frame {out_frame_idx + 1}/{num_frames}\")\n",
        "\n",
        "    print(f\"‚úÖ Ph∆∞∆°ng ph√°p 1 th√†nh c√¥ng! C√≥ {len(video_segments)} frames v·ªõi masks\")\n",
        "\n",
        "except (RuntimeError, Exception) as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Ph∆∞∆°ng ph√°p 1 b·ªã l·ªói: {str(e)[:100]}...\")\n",
        "    print(\"üîÑ Chuy·ªÉn sang ph∆∞∆°ng ph√°p 2: Propagate t·ª´ng object ri√™ng bi·ªát (T·ªêI ∆ØU)...\\n\")\n",
        "\n",
        "    # ============================================================\n",
        "    # PH∆Ø∆†NG PH√ÅP 2: Propagate t·ª´ng object ri√™ng bi·ªát v·ªõi gi·ªõi h·∫°n frames\n",
        "    # ============================================================\n",
        "    video_segments = {}  # Reset\n",
        "\n",
        "    # S·∫Øp x·∫øp objects theo frame_idx ƒë·ªÉ x√°c ƒë·ªãnh object cu·ªëi\n",
        "    sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1][\"frame_idx\"])\n",
        "    total_objects = len(sorted_prompts)\n",
        "\n",
        "    for obj_counter, (obj_id, info) in enumerate(sorted_prompts, 1):\n",
        "        frame_idx = info[\"frame_idx\"]\n",
        "        x, y = info[\"point\"]\n",
        "\n",
        "        # T√≠nh to√°n frame k·∫øt th√∫c\n",
        "        # Object cu·ªëi c√πng ho·∫∑c c√°c object g·∫ßn cu·ªëi -> propagate ƒë·∫øn h·∫øt video\n",
        "        remaining_objects = total_objects - obj_counter\n",
        "        if remaining_objects <= 2:  # 2 objects cu·ªëi c√πng propagate ƒë·∫øn h·∫øt\n",
        "            end_frame = num_frames\n",
        "        else:\n",
        "            end_frame = min(frame_idx + FRAMES_PER_OBJECT, num_frames)\n",
        "\n",
        "        frames_to_process = end_frame - frame_idx\n",
        "        print(f\"   [{obj_counter}/{total_objects}] Object {obj_id}: frame {frame_idx} ‚Üí {end_frame-1} ({frames_to_process} frames)\")\n",
        "\n",
        "        try:\n",
        "            # Reset v√† kh·ªüi t·∫°o l·∫°i inference state cho object n√†y\n",
        "            predictor.reset_state(inference_state)\n",
        "\n",
        "            # Th√™m prompt cho object n√†y\n",
        "            points = np.array([[x, y]], dtype=np.float32)\n",
        "            labels = np.array([1], dtype=np.int32)\n",
        "\n",
        "            predictor.add_new_points_or_box(\n",
        "                inference_state=inference_state,\n",
        "                frame_idx=frame_idx,\n",
        "                obj_id=obj_id,\n",
        "                points=points,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            # Propagate object n√†y - CH·ªà L·∫§Y FRAMES TRONG KHO·∫¢NG GI·ªöI H·∫†N\n",
        "            for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "                # Ch·ªâ l∆∞u mask n·∫øu frame n·∫±m trong kho·∫£ng cho ph√©p\n",
        "                if out_frame_idx < frame_idx or out_frame_idx >= end_frame:\n",
        "                    continue\n",
        "\n",
        "                if out_frame_idx not in video_segments:\n",
        "                    video_segments[out_frame_idx] = {}\n",
        "\n",
        "                for i in range(len(out_obj_ids)):\n",
        "                    out_obj_id = out_obj_ids[i]\n",
        "                    if hasattr(out_obj_id, 'item'):\n",
        "                        out_obj_id = out_obj_id.item()\n",
        "                    else:\n",
        "                        out_obj_id = int(out_obj_id)\n",
        "\n",
        "                    if out_obj_id == obj_id:\n",
        "                        mask = (out_mask_logits[i] > 0.0).cpu().numpy().squeeze()\n",
        "                        video_segments[out_frame_idx][obj_id] = mask\n",
        "                        break\n",
        "\n",
        "            print(f\"      ‚úÖ Object {obj_id} ho√†n t·∫•t\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"      ‚ö†Ô∏è Object {obj_id} b·ªã l·ªói: {str(e2)[:80]}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n‚úÖ Ph∆∞∆°ng ph√°p 2 ho√†n t·∫•t! C√≥ {len(video_segments)} frames v·ªõi masks\")\n",
        "\n",
        "# X√≥a frames r·ªóng (kh√¥ng c√≥ object n√†o)\n",
        "frames_to_remove = [f for f in video_segments if len(video_segments[f]) == 0]\n",
        "for f in frames_to_remove:\n",
        "    del video_segments[f]\n",
        "\n",
        "print(f\"\\nüìä Th·ªëng k√™ cu·ªëi c√πng:\")\n",
        "print(f\"   - T·ªïng frames c√≥ objects: {len(video_segments)}\")\n",
        "print(f\"   - T·ªïng objects: {len(all_prompts)}\")\n",
        "if video_segments:\n",
        "    total_mask_count = sum(len(masks) for masks in video_segments.values())\n",
        "    print(f\"   - T·ªïng masks ƒë√£ t·∫°o: {total_mask_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y93-Mwsm55PU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y93-Mwsm55PU",
        "outputId": "4fee7301-26ec-489c-e761-46abe2384fda"
      },
      "outputs": [],
      "source": [
        "# Propagate masks qua to√†n b·ªô video\n",
        "# T·ªêI ∆ØU: M·ªói object ch·ªâ propagate trong kho·∫£ng frame gi·ªõi h·∫°n\n",
        "# ‚úÖ CH·ªà CH·∫†Y PH∆Ø∆†NG PH√ÅP 2: Propagate t·ª´ng object ri√™ng bi·ªát + D·ª™NG S·ªöM\n",
        "\n",
        "print(\"üîÑ ƒêang propagate masks qua to√†n b·ªô video...\")\n",
        "print(\"   (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t t√πy thu·ªôc v√†o ƒë·ªô d√†i video)\")\n",
        "print(\"   üìå Mode: Propagate t·ª´ng object ri√™ng bi·ªát (Ph∆∞∆°ng ph√°p 2 + Early Stop)\\n\")\n",
        "\n",
        "video_segments = {}  # {frame_idx: {obj_id: mask}}\n",
        "\n",
        "# ============================================================\n",
        "# C·∫§U H√åNH\n",
        "# ============================================================\n",
        "FRAMES_PER_OBJECT = 65  # S·ªë frames t·ªëi ƒëa ƒë·ªÉ propagate cho m·ªói object\n",
        "\n",
        "# ============================================================\n",
        "# PH∆Ø∆†NG PH√ÅP 2: Propagate t·ª´ng object ri√™ng bi·ªát v·ªõi EARLY STOP\n",
        "# ============================================================\n",
        "\n",
        "# S·∫Øp x·∫øp objects theo frame_idx ƒë·ªÉ x√°c ƒë·ªãnh object cu·ªëi\n",
        "sorted_prompts = sorted(all_prompts.items(), key=lambda x: x[1][\"frame_idx\"])\n",
        "total_objects = len(sorted_prompts)\n",
        "\n",
        "print(f\"üìä T·ªïng s·ªë objects: {total_objects}\")\n",
        "print(f\"üìä T·ªïng s·ªë frames: {num_frames}\")\n",
        "print(f\"üìä Frames per object: {FRAMES_PER_OBJECT}\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for obj_counter, (obj_id, info) in enumerate(sorted_prompts, 1):\n",
        "    frame_idx = info[\"frame_idx\"]\n",
        "    x, y = info[\"point\"]\n",
        "\n",
        "    # T√≠nh to√°n frame k·∫øt th√∫c\n",
        "    remaining_objects = total_objects - obj_counter\n",
        "    if remaining_objects <= 2:  # 2 objects cu·ªëi c√πng propagate ƒë·∫øn h·∫øt\n",
        "        end_frame = num_frames\n",
        "    else:\n",
        "        end_frame = min(frame_idx + FRAMES_PER_OBJECT, num_frames)\n",
        "\n",
        "    frames_to_process = end_frame - frame_idx\n",
        "    print(f\"\\n[{obj_counter}/{total_objects}] Object {obj_id}\")\n",
        "    print(f\"   üìç Xu·∫•t hi·ªán t·∫°i frame: {frame_idx}\")\n",
        "    print(f\"   üìç T·ªça ƒë·ªô: ({x}, {y})\")\n",
        "    print(f\"   üîÑ Propagate t·ª´ frame {frame_idx} ‚Üí {end_frame-1} ({frames_to_process} frames)\")\n",
        "\n",
        "    try:\n",
        "        # Kh·ªüi t·∫°o l·∫°i inference state cho object n√†y\n",
        "        inference_state = predictor.init_state(video_path=frames_dir)\n",
        "\n",
        "        # Th√™m prompt cho object n√†y\n",
        "        points = np.array([[x, y]], dtype=np.float32)\n",
        "        labels = np.array([1], dtype=np.int32)\n",
        "\n",
        "        predictor.add_new_points_or_box(\n",
        "            inference_state=inference_state,\n",
        "            frame_idx=frame_idx,\n",
        "            obj_id=obj_id,\n",
        "            points=points,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        # Propagate object n√†y v·ªõi EARLY STOP\n",
        "        masks_found = 0\n",
        "        for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "            # ‚ö° EARLY STOP: D·ª´ng ngay khi v∆∞·ª£t qu√° end_frame\n",
        "            if out_frame_idx >= end_frame:\n",
        "                print(f\"   ‚ö° Early stop t·∫°i frame {out_frame_idx}\")\n",
        "                break\n",
        "\n",
        "            # B·ªè qua frames tr∆∞·ªõc frame_idx (n·∫øu c√≥)\n",
        "            if out_frame_idx < frame_idx:\n",
        "                continue\n",
        "\n",
        "            if out_frame_idx not in video_segments:\n",
        "                video_segments[out_frame_idx] = {}\n",
        "\n",
        "            for i in range(len(out_obj_ids)):\n",
        "                out_obj_id = out_obj_ids[i]\n",
        "                if hasattr(out_obj_id, 'item'):\n",
        "                    out_obj_id = out_obj_id.item()\n",
        "                else:\n",
        "                    out_obj_id = int(out_obj_id)\n",
        "\n",
        "                if out_obj_id == obj_id:\n",
        "                    mask = (out_mask_logits[i] > 0.0).cpu().numpy().squeeze()\n",
        "                    video_segments[out_frame_idx][obj_id] = mask\n",
        "                    masks_found += 1\n",
        "                    break\n",
        "\n",
        "        print(f\"   ‚úÖ Ho√†n t·∫•t - T√¨m ƒë∆∞·ª£c {masks_found} masks\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"   ‚ö†Ô∏è L·ªói: {str(e2)[:100]}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# X√≥a frames r·ªóng (kh√¥ng c√≥ object n√†o)\n",
        "frames_to_remove = [f for f in video_segments if len(video_segments[f]) == 0]\n",
        "for f in frames_to_remove:\n",
        "    del video_segments[f]\n",
        "\n",
        "print(f\"\\n‚úÖ Propagation ho√†n t·∫•t!\\n\")\n",
        "print(f\"üìä Th·ªëng k√™ cu·ªëi c√πng:\")\n",
        "print(f\"   - T·ªïng frames c√≥ objects: {len(video_segments)}\")\n",
        "print(f\"   - T·ªïng objects: {len(all_prompts)}\")\n",
        "if video_segments:\n",
        "    total_mask_count = sum(len(masks) for masks in video_segments.values())\n",
        "    print(f\"   - T·ªïng masks ƒë√£ t·∫°o: {total_mask_count}\")\n",
        "\n",
        "    # Chi ti·∫øt t·ª´ng object\n",
        "    print(f\"\\nüìã Chi ti·∫øt objects:\")\n",
        "    for obj_id in sorted(all_prompts.keys()):\n",
        "        frames_with_obj = [f for f in video_segments if obj_id in video_segments[f]]\n",
        "        print(f\"   Object {obj_id}: {len(frames_with_obj)} frames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb54a87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4cb54a87",
        "outputId": "83b66ec1-2649-4376-921b-b05e50ce6771"
      },
      "outputs": [],
      "source": [
        "# Visualize k·∫øt qu·∫£ tr√™n m·ªói 10 frames\n",
        "print(\"üé¨ Visualize k·∫øt qu·∫£ tracking qua video...\\n\")\n",
        "\n",
        "# T·∫°o danh s√°ch frames ƒë·ªÉ visualize (m·ªói 10 frames)\n",
        "vis_frames = list(range(0, num_frames, 10))\n",
        "if (num_frames - 1) not in vis_frames:\n",
        "    vis_frames.append(num_frames - 1)  # Th√™m frame cu·ªëi\n",
        "\n",
        "# T√≠nh s·ªë h√†ng c·∫ßn thi·∫øt (m·ªói h√†ng 5 ·∫£nh)\n",
        "cols = 5\n",
        "rows = (len(vis_frames) + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
        "axes = axes.flatten() if rows > 1 else (axes if cols > 1 else [axes])\n",
        "\n",
        "for idx, frame_idx in enumerate(vis_frames):\n",
        "    if idx >= len(axes):\n",
        "        break\n",
        "\n",
        "    ax = axes[idx]\n",
        "    frame_path = os.path.join(frames_dir, frame_files[frame_idx])\n",
        "    frame = np.array(Image.open(frame_path))\n",
        "    ax.imshow(frame)\n",
        "\n",
        "    # ƒê·∫øm s·ªë objects trong frame n√†y\n",
        "    obj_count = 0\n",
        "    if frame_idx in video_segments:\n",
        "        for obj_id, mask in video_segments[frame_idx].items():\n",
        "            show_mask(mask, ax, obj_id)\n",
        "            obj_count += 1\n",
        "\n",
        "    ax.set_title(f\"Frame {frame_idx}\\n({obj_count} objects)\", fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "# ·∫®n c√°c axes th·ª´a\n",
        "for idx in range(len(vis_frames), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "    axes[idx].set_visible(False)\n",
        "\n",
        "plt.suptitle(\"‚öôÔ∏è K·∫øt qu·∫£ Tracking ƒê·ªëi t∆∞·ª£ng qua Video (m·ªói 10 frames)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Th·ªëng k√™ chi ti·∫øt\n",
        "print(\"\\nüìä Th·ªëng k√™ chi ti·∫øt theo frame:\")\n",
        "print(\"-\" * 50)\n",
        "for frame_idx in vis_frames:\n",
        "    if frame_idx in video_segments:\n",
        "        obj_ids = list(video_segments[frame_idx].keys())\n",
        "        print(f\"   Frame {frame_idx:3d}: {len(obj_ids)} objects - IDs: {obj_ids}\")\n",
        "    else:\n",
        "        print(f\"   Frame {frame_idx:3d}: 0 objects\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7a061b5",
      "metadata": {
        "id": "e7a061b5"
      },
      "source": [
        "## 5. Convert sang YOLO Format\n",
        "\n",
        "Chuy·ªÉn ƒë·ªïi segmentation masks sang format YOLO detection/segmentation ƒë·ªÉ training model.\n",
        "\n",
        "**YOLO Format:** `class_id x_center y_center width height` (normalized 0-1) ho·∫∑c `class_id x1 y1 x2 y2 ...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8725481d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8725481d",
        "outputId": "70743e67-8631-422d-c8ba-9a1b40f0b5b3"
      },
      "outputs": [],
      "source": [
        "def mask_to_yolo_bbox(mask, image_width, image_height, class_id=0):\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn ƒë·ªïi binary mask sang YOLO bounding box format.\n",
        "\n",
        "    Args:\n",
        "        mask: 2D numpy array (H, W) v·ªõi True/False ho·∫∑c 1/0\n",
        "        image_width: Chi·ªÅu r·ªông ·∫£nh\n",
        "        image_height: Chi·ªÅu cao ·∫£nh\n",
        "        class_id: ID c·ªßa class (m·∫∑c ƒë·ªãnh 0 cho 'product')\n",
        "\n",
        "    Returns:\n",
        "        String \"class_id x_center y_center width height\" ho·∫∑c None n·∫øu mask r·ªóng\n",
        "    \"\"\"\n",
        "    # T√¨m v·ªã tr√≠ c√°c pixel trong mask\n",
        "    rows = np.any(mask, axis=1)\n",
        "    cols = np.any(mask, axis=0)\n",
        "\n",
        "    if not rows.any() or not cols.any():\n",
        "        return None  # Mask r·ªóng\n",
        "\n",
        "    # T√¨m bounding box\n",
        "    y_indices = np.where(rows)[0]\n",
        "    x_indices = np.where(cols)[0]\n",
        "    y_min, y_max = y_indices[0], y_indices[-1]\n",
        "    x_min, x_max = x_indices[0], x_indices[-1]\n",
        "\n",
        "    # Chuy·ªÉn sang YOLO format (normalized center + size)\n",
        "    x_center = ((x_min + x_max) / 2) / image_width\n",
        "    y_center = ((y_min + y_max) / 2) / image_height\n",
        "    width = (x_max - x_min) / image_width\n",
        "    height = (y_max - y_min) / image_height\n",
        "\n",
        "    return f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "\n",
        "\n",
        "def mask_to_yolo_segmentation(mask, image_width, image_height, class_id=0):\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn ƒë·ªïi binary mask sang YOLO segmentation format (polygon).\n",
        "\n",
        "    Args:\n",
        "        mask: 2D numpy array (H, W) v·ªõi True/False ho·∫∑c 1/0\n",
        "        image_width: Chi·ªÅu r·ªông ·∫£nh\n",
        "        image_height: Chi·ªÅu cao ·∫£nh\n",
        "        class_id: ID c·ªßa class\n",
        "\n",
        "    Returns:\n",
        "        String \"class_id x1 y1 x2 y2 ... xn yn\" (normalized) ho·∫∑c None\n",
        "    \"\"\"\n",
        "    # Chuy·ªÉn mask sang uint8\n",
        "    mask_uint8 = (mask * 255).astype(np.uint8)\n",
        "\n",
        "    # T√¨m contours\n",
        "    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        return None\n",
        "\n",
        "    # L·∫•y contour l·ªõn nh·∫•t\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # ƒê∆°n gi·∫£n h√≥a contour ƒë·ªÉ gi·∫£m s·ªë ƒëi·ªÉm\n",
        "    epsilon = 0.005 * cv2.arcLength(largest_contour, True)\n",
        "    approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
        "\n",
        "    if len(approx) < 3:\n",
        "        return None  # C·∫ßn √≠t nh·∫•t 3 ƒëi·ªÉm cho polygon\n",
        "\n",
        "    # Chuy·ªÉn sang normalized coordinates\n",
        "    points = approx.squeeze()\n",
        "    normalized_points = []\n",
        "    for point in points:\n",
        "        x_norm = point[0] / image_width\n",
        "        y_norm = point[1] / image_height\n",
        "        normalized_points.extend([f\"{x_norm:.6f}\", f\"{y_norm:.6f}\"])\n",
        "\n",
        "    return f\"{class_id} \" + \" \".join(normalized_points)\n",
        "\n",
        "\n",
        "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m chuy·ªÉn ƒë·ªïi YOLO format\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb98ba49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb98ba49",
        "outputId": "7b1ac24b-1c82-4bae-8bb4-cd04b925a96d"
      },
      "outputs": [],
      "source": [
        "# C·∫•u h√¨nh output\n",
        "OUTPUT_FORMAT = \"detection\"  # \"detection\" (bbox) ho·∫∑c \"segmentation\" (polygon)\n",
        "CLASS_ID = 0  # Class ID cho ƒë·ªëi t∆∞·ª£ng/s·∫£n ph·∫©m\n",
        "TRAIN_SPLIT = 0.8  # 80% train, 20% val\n",
        "\n",
        "# T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c YOLO\n",
        "output_dir = \"yolo_product_dataset\"\n",
        "os.makedirs(f\"{output_dir}/images/train\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/images/val\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/labels/train\", exist_ok=True)\n",
        "os.makedirs(f\"{output_dir}/labels/val\", exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ ƒê√£ t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c YOLO t·∫°i '{output_dir}/'\")\n",
        "print(f\"   - Format: {OUTPUT_FORMAT}\")\n",
        "print(f\"   - Train/Val split: {TRAIN_SPLIT*100:.0f}%/{(1-TRAIN_SPLIT)*100:.0f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db20cdac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db20cdac",
        "outputId": "60e7ba35-c90a-4374-88b5-95b67c870b65"
      },
      "outputs": [],
      "source": [
        "# Export annotations sang YOLO format\n",
        "# ‚è±Ô∏è KH√îNG RANDOM - Gi·ªØ th·ª© t·ª± th·ªùi gian c·ªßa video (sequential split)\n",
        "import shutil\n",
        "\n",
        "# ============================================================\n",
        "# C·∫§U H√åNH TRAIN/VAL SPLIT THEO TH·ª® T·ª∞ TH·ªúI GIAN\n",
        "# ============================================================\n",
        "frame_indices = sorted(list(video_segments.keys()))  # Gi·ªØ nguy√™n th·ª© t·ª±\n",
        "\n",
        "# Split train/val theo th·ª© t·ª± tu·∫ßn t·ª± (80% tr∆∞·ªõc, 20% sau)\n",
        "split_idx = int(len(frame_indices) * TRAIN_SPLIT)\n",
        "train_indices = set(frame_indices[:split_idx])\n",
        "val_indices = set(frame_indices[split_idx:])\n",
        "\n",
        "stats = {\"train\": 0, \"val\": 0, \"total_objects\": 0, \"empty_frames\": 0}\n",
        "\n",
        "print(\"üìù ƒêang export annotations...\")\n",
        "print(f\"   üìä Train/Val split (Sequential - theo th·ªùi gian):\")\n",
        "print(f\"      - Train: frames {frame_indices[0]} ‚Üí {frame_indices[split_idx-1]} ({len(train_indices)} frames)\")\n",
        "print(f\"      - Val: frames {frame_indices[split_idx]} ‚Üí {frame_indices[-1]} ({len(val_indices)} frames)\\n\")\n",
        "\n",
        "for frame_idx in sorted(video_segments.keys()):\n",
        "    # X√°c ƒë·ªãnh train hay val\n",
        "    split = \"train\" if frame_idx in train_indices else \"val\"\n",
        "\n",
        "    # ƒê∆∞·ªùng d·∫´n files\n",
        "    frame_name = frame_files[frame_idx]\n",
        "    label_name = frame_name.replace('.jpg', '.txt')\n",
        "\n",
        "    src_image = os.path.join(frames_dir, frame_name)\n",
        "    dst_image = os.path.join(output_dir, \"images\", split, frame_name)\n",
        "    dst_label = os.path.join(output_dir, \"labels\", split, label_name)\n",
        "\n",
        "    # Copy image\n",
        "    shutil.copy2(src_image, dst_image)\n",
        "\n",
        "    # T·∫°o label file\n",
        "    labels = []\n",
        "    for obj_id, mask in video_segments[frame_idx].items():\n",
        "        if OUTPUT_FORMAT == \"detection\":\n",
        "            label = mask_to_yolo_bbox(mask, frame_width, frame_height, CLASS_ID)\n",
        "        else:\n",
        "            label = mask_to_yolo_segmentation(mask, frame_width, frame_height, CLASS_ID)\n",
        "\n",
        "        if label:\n",
        "            labels.append(label)\n",
        "            stats[\"total_objects\"] += 1\n",
        "\n",
        "    # Ghi label file\n",
        "    if labels:\n",
        "        with open(dst_label, 'w') as f:\n",
        "            f.write('\\n'.join(labels))\n",
        "        stats[split] += 1\n",
        "    else:\n",
        "        # T·∫°o file r·ªóng cho frames kh√¥ng c√≥ object\n",
        "        with open(dst_label, 'w') as f:\n",
        "            pass\n",
        "        stats[\"empty_frames\"] += 1\n",
        "\n",
        "    # Progress\n",
        "    if (frame_idx + 1) % 20 == 0:\n",
        "        print(f\"   Processed {frame_idx + 1}/{len(video_segments)} frames\")\n",
        "\n",
        "print(f\"\\n‚úÖ Export ho√†n t·∫•t!\")\n",
        "print(f\"   üìä Th·ªëng k√™:\")\n",
        "print(f\"      - Train images: {stats['train']}\")\n",
        "print(f\"      - Val images: {stats['val']}\")\n",
        "print(f\"      - T·ªïng s·ªë objects: {stats['total_objects']}\")\n",
        "print(f\"      - Empty frames: {stats['empty_frames']}\")\n",
        "print(f\"\\n   ‚è±Ô∏è Th·ª© t·ª± th·ªùi gian ƒë∆∞·ª£c gi·ªØ nguy√™n:\")\n",
        "print(f\"      Train: nh·ªØng frame s·ªõm trong video\")\n",
        "print(f\"      Val: nh·ªØng frame mu·ªôn trong video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973554e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "973554e4",
        "outputId": "333175fd-a1f0-4ab9-b405-9e2a02bac994"
      },
      "outputs": [],
      "source": [
        "# T·∫°o file data.yaml cho YOLO training\n",
        "data_yaml_content = f\"\"\"# Industrial Product Detection Dataset\n",
        "# Generated by SAM 2 Object Tracking Notebook\n",
        "\n",
        "path: {os.path.abspath(output_dir)}\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: product\n",
        "\n",
        "# Number of classes\n",
        "nc: 1\n",
        "\"\"\"\n",
        "\n",
        "yaml_path = os.path.join(output_dir, \"data.yaml\")\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(data_yaml_content)\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ t·∫°o file data.yaml\")\n",
        "print(f\"\\nüìÑ N·ªôi dung data.yaml:\")\n",
        "print(\"-\" * 40)\n",
        "print(data_yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0e4e62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e0e4e62",
        "outputId": "1245d47a-2d87-45a3-b6e1-16956e85c323"
      },
      "outputs": [],
      "source": [
        "# Verify m·ªôt s·ªë annotations\n",
        "print(\"üîç Ki·ªÉm tra m·ªôt s·ªë annotations m·∫´u:\\n\")\n",
        "\n",
        "# L·∫•y m·ªôt v√†i file label ƒë·ªÉ verify\n",
        "sample_labels = []\n",
        "for split in [\"train\", \"val\"]:\n",
        "    label_dir = os.path.join(output_dir, \"labels\", split)\n",
        "    labels = [f for f in os.listdir(label_dir) if f.endswith('.txt')]\n",
        "    if labels:\n",
        "        sample_labels.append((split, labels[0]))\n",
        "\n",
        "for split, label_file in sample_labels[:2]:\n",
        "    label_path = os.path.join(output_dir, \"labels\", split, label_file)\n",
        "    image_file = label_file.replace('.txt', '.jpg')\n",
        "    image_path = os.path.join(output_dir, \"images\", split, image_file)\n",
        "\n",
        "    print(f\"üìÅ {split}/{label_file}:\")\n",
        "    with open(label_path, 'r') as f:\n",
        "        content = f.read().strip()\n",
        "        if content:\n",
        "            lines = content.split('\\n')\n",
        "            print(f\"   S·ªë objects: {len(lines)}\")\n",
        "            for i, line in enumerate(lines[:3], 1):  # Show max 3 lines\n",
        "                print(f\"   Line {i}: {line[:80]}...\")\n",
        "            if len(lines) > 3:\n",
        "                print(f\"   ... v√† {len(lines) - 3} objects kh√°c\")\n",
        "        else:\n",
        "            print(\"   (Empty - no objects)\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd2a53e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "id": "0cd2a53e",
        "outputId": "f4c62fc8-e79a-4298-cbb3-49c70e6e0e0c"
      },
      "outputs": [],
      "source": [
        "# Visualize annotation tr√™n m·ªôt frame m·∫´u\n",
        "def visualize_yolo_annotation(image_path, label_path, title=\"\"):\n",
        "    \"\"\"Hi·ªÉn th·ªã ·∫£nh v·ªõi bounding boxes t·ª´ YOLO annotation\"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # ƒê·ªçc labels\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) >= 5:  # Detection format\n",
        "            class_id = int(parts[0])\n",
        "            x_center, y_center, width, height = map(float, parts[1:5])\n",
        "\n",
        "            # Convert to pixel coordinates\n",
        "            x1 = (x_center - width/2) * w\n",
        "            y1 = (y_center - height/2) * h\n",
        "            box_w = width * w\n",
        "            box_h = height * h\n",
        "\n",
        "            # Draw rectangle\n",
        "            rect = plt.Rectangle((x1, y1), box_w, box_h,\n",
        "                                 fill=False, edgecolor=colors[i % 20], linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(x1, y1 - 5, f\"product_{i+1}\", color=colors[i % 20],\n",
        "                   fontsize=8, fontweight='bold')\n",
        "\n",
        "    ax.set_title(f\"{title}\\n({len(lines)} objects detected)\")\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize sample\n",
        "for split in [\"train\", \"val\"]:\n",
        "    label_dir = os.path.join(output_dir, \"labels\", split)\n",
        "    labels = sorted([f for f in os.listdir(label_dir) if f.endswith('.txt')])\n",
        "\n",
        "    # T√¨m file c√≥ annotations\n",
        "    for label_file in labels[:5]:\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        with open(label_path, 'r') as f:\n",
        "            if f.read().strip():  # C√≥ content\n",
        "                image_file = label_file.replace('.txt', '.jpg')\n",
        "                image_path = os.path.join(output_dir, \"images\", split, image_file)\n",
        "                visualize_yolo_annotation(image_path, label_path, f\"{split.upper()}: {image_file}\")\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083a2c2b",
      "metadata": {
        "id": "083a2c2b"
      },
      "source": [
        "## 6. Download Dataset (Colab)\n",
        "\n",
        "T·∫£i dataset v·ªÅ m√°y local ho·∫∑c mount Google Drive ƒë·ªÉ l∆∞u tr·ªØ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hhZsVmN1HUN3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hhZsVmN1HUN3",
        "outputId": "6cf91eba-f9e9-4ef6-f683-e5cfbb6b3195"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "output_dir = \"yolo_product_dataset\"\n",
        "zip_filename = \"industrial_product_yolo_dataset.zip\"\n",
        "\n",
        "# N√©n dataset th√†nh file zip\n",
        "!cd {output_dir} && zip -r ../{zip_filename} . -q\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ t·∫°o file {zip_filename}\")\n",
        "\n",
        "# Download (ch·ªâ tr√™n Colab)\n",
        "if IN_COLAB:\n",
        "    files.download(zip_filename)\n",
        "    print(\"üì• ƒêang t·∫£i xu·ªëng...\")\n",
        "else:\n",
        "    print(f\"üìÅ Dataset ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.abspath(output_dir)}\")\n",
        "    print(f\"üì¶ File zip: {os.path.abspath(zip_filename)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89f81c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "b89f81c8",
        "outputId": "cc0d3487-5466-46a8-89b3-0fdfb43c6532"
      },
      "outputs": [],
      "source": [
        "# N√©n dataset th√†nh file zip\n",
        "zip_filename = \"industrial_product_yolo_dataset.zip\"\n",
        "!cd {output_dir} && zip -r ../{zip_filename} . -q\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ t·∫°o file {zip_filename}\")\n",
        "\n",
        "# Download (ch·ªâ tr√™n Colab)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    print(\"üì• ƒêang t·∫£i xu·ªëng...\")\n",
        "else:\n",
        "    print(f\"üìÅ Dataset ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {os.path.abspath(output_dir)}\")\n",
        "    print(f\"üì¶ File zip: {os.path.abspath(zip_filename)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b448c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7b448c2",
        "outputId": "a3f99e00-da9d-40b5-c940-247293bc3022"
      },
      "outputs": [],
      "source": [
        "# (Optional) L∆∞u v√†o Google Drive\n",
        "if IN_COLAB:\n",
        "    save_to_drive = input(\"B·∫°n c√≥ mu·ªën l∆∞u v√†o Google Drive? (y/n): \").lower() == 'y'\n",
        "\n",
        "    if save_to_drive:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        drive_path = \"/content/drive/MyDrive/industrial_product_dataset\"\n",
        "        !mkdir -p {drive_path}\n",
        "        !cp -r {output_dir}/* {drive_path}/\n",
        "        !cp {zip_filename} {drive_path}/\n",
        "\n",
        "        print(f\"‚úÖ ƒê√£ l∆∞u dataset v√†o Google Drive: {drive_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef04b19f",
      "metadata": {
        "id": "ef04b19f"
      },
      "source": [
        "## 7. (Bonus) Train YOLO Model\n",
        "\n",
        "Sau khi c√≥ dataset, b·∫°n c√≥ th·ªÉ train model YOLO ngay tr√™n Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7313af1d",
      "metadata": {
        "id": "7313af1d"
      },
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t Ultralytics YOLO\n",
        "!pip install -q ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load pretrained YOLOv8 model\n",
        "model = YOLO('yolov8n.pt')  # nano model, nhanh nh·∫•t\n",
        "# C√≥ th·ªÉ d√πng: yolov8s.pt (small), yolov8m.pt (medium), yolov8l.pt (large)\n",
        "\n",
        "print(\"‚úÖ ƒê√£ load YOLOv8 model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bec55a3",
      "metadata": {
        "id": "3bec55a3"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "# B·ªè comment d√≤ng d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu training\n",
        "\n",
        "# results = model.train(\n",
        "#     data=os.path.join(output_dir, \"data.yaml\"),\n",
        "#     epochs=50,\n",
        "#     imgsz=640,\n",
        "#     batch=16,\n",
        "#     name=\"product_detector\",\n",
        "#     patience=10,\n",
        "#     device=0 if torch.cuda.is_available() else 'cpu',\n",
        "# )\n",
        "\n",
        "print(\"üí° B·ªè comment code ph√≠a tr√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu training YOLO model\")\n",
        "print(\"‚è±Ô∏è Training c√≥ th·ªÉ m·∫•t 30-60 ph√∫t t√πy thu·ªôc v√†o GPU v√† s·ªë l∆∞·ª£ng data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ebc2d9d",
      "metadata": {
        "id": "3ebc2d9d"
      },
      "source": [
        "---\n",
        "\n",
        "## üìã T√≥m t·∫Øt\n",
        "\n",
        "Notebook n√†y ƒë√£ th·ª±c hi·ªán:\n",
        "\n",
        "1. ‚úÖ **Setup m√¥i tr∆∞·ªùng** - C√†i ƒë·∫∑t SAM 2 v√† dependencies tr√™n Colab\n",
        "2. ‚úÖ **Video Processing** - Extract video th√†nh frames JPEG\n",
        "3. ‚úÖ **Interactive Annotation** - Giao di·ªán click ch·ªçn ƒë·ªëi t∆∞·ª£ng\n",
        "4. ‚úÖ **SAM 2 Segmentation** - Segment v√† tracking t·ª± ƒë·ªông qua video\n",
        "5. ‚úÖ **YOLO Export** - Xu·∫•t dataset theo format YOLO (detection/segmentation)\n",
        "6. ‚úÖ **Download** - T·∫£i dataset v·ªÅ ho·∫∑c l∆∞u v√†o Google Drive\n",
        "\n",
        "### üìÅ C·∫•u tr√∫c Dataset Output:\n",
        "```\n",
        "yolo_product_dataset/\n",
        "‚îú‚îÄ‚îÄ images/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00000.jpg\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
        "‚îú‚îÄ‚îÄ labels/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00000.txt\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
        "‚îî‚îÄ‚îÄ data.yaml\n",
        "```\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "- Train YOLO model v·ªõi dataset ƒë√£ t·∫°o\n",
        "- Fine-tune hyperparameters n·∫øu k·∫øt qu·∫£ ch∆∞a t·ªët\n",
        "- Augment data n·∫øu c·∫ßn th√™m samples"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
